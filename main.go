package main

import (
	"encoding/json"
	"errors"
	"fmt"
	"log"
	"net/http"
	"os"
	"os/signal"
	"strings"
	"sync"
	"syscall"
	"time"

	mqtt "github.com/eclipse/paho.mqtt.golang"
	"github.com/joho/godotenv"
	"github.com/nedpals/supabase-go"
)

// DeviceLog represents the structure of a device log entry
type DeviceLog struct {
	ID                 string     `json:"id"`                   // UUID, auto-generated by Supabase
	DeviceID           string     `json:"device_id"`            // UUID
	UserID             string     `json:"user_id"`              // UUID
	CreatedAt          time.Time  `json:"created_at"`           // Timestamp
	Status             string     `json:"status"`               // e.g., "online", "offline", "error"
	TempSensorReading  *float64   `json:"temp_sensor_reading"`  // Nullable
	HumidSensorReading *float64   `json:"humid_sensor_reading"` // Nullable
	RelayState         string     `json:"relay_state"`          // e.g., "on", "off", "idle"
	DeviceType         string     `json:"device_type"`          // "relay" or "sensor"
	Metadata           *string    `json:"metadata"`             // Optional JSON metadata
}

// Task structure to represent an MQTT message
type Task struct {
	Topic   string
	Payload string
}

// Global variables
var (
	taskQueue         = make(chan Task, 1000) // Buffered channel to hold tasks
	maxWorkers        = 20                    // Maximum number of workers
	minWorkers        = 2                     // Minimum number of workers
	workerLock        sync.Mutex              // Mutex to protect workerCount
	activeWorkers     = make(map[int]chan bool)
	workerIDCounter   = 0
	supabaseClient    *supabase.Client
	wg                sync.WaitGroup
	gracefulShutdown  = make(chan os.Signal, 1) // Channel to capture OS signals
	once              sync.Once                   // Ensures Supabase client is initialized once
	defaultDeviceID   = "default-device-id"       // Default device_id for plain text payloads
	defaultDeviceType = "relay"                   // Default device_type for plain text payloads
)


var (
	mqttClientInstance mqtt.Client
	mqttOnce           sync.Once
)


func getMQTTClient() mqtt.Client {
	mqttOnce.Do(func() {
		broker := "tcp://mosquitto:1883"
		clientID := "guardian-daemon"

		opts := mqtt.NewClientOptions().
			AddBroker(broker).
			SetClientID(clientID).
			SetDefaultPublishHandler(messageHandler)

		client := mqtt.NewClient(opts)
		if token := client.Connect(); token.Wait() && token.Error() != nil {
			log.Fatalf("Failed to connect to MQTT broker: %v", token.Error())
		}
		log.Println("Connected to MQTT broker!")
		mqttClientInstance = client
	})
	return mqttClientInstance
}

func handleSendCommand(w http.ResponseWriter, r *http.Request) {
    if r.Method != http.MethodPost {
        http.Error(w, "Invalid request method. Only POST is allowed.", http.StatusMethodNotAllowed)
        return
    }

    // Parse the incoming JSON
    var message struct {
        DeviceID string                 `json:"deviceId"`
        UserID   string                 `json:"userId"`
        Data     map[string]interface{} `json:"data"` // Dynamic key-value structure
    }
    if err := json.NewDecoder(r.Body).Decode(&message); err != nil {
        log.Printf("Error decoding request body: %v", err)
        http.Error(w, "Invalid JSON format.", http.StatusBadRequest)
        return
    }
    defer r.Body.Close()

    // Validate required fields
    if message.DeviceID == "" || message.UserID == "" || message.Data == nil {
        http.Error(w, "Missing required fields: deviceId, userId, or data.", http.StatusBadRequest)
        return
    }

    // Serialize the Data object into a JSON string
    dataJSON, err := json.Marshal(message.Data)
    if err != nil {
        log.Printf("Error serializing data to JSON: %v", err)
        http.Error(w, "Failed to serialize data to JSON.", http.StatusInternalServerError)
        return
    }

    // Construct the MQTT topic
    topic := fmt.Sprintf("/toDevice/%s/%s", message.UserID, message.DeviceID)

    // Publish the message to MQTT
    client := getMQTTClient() // Ensure singleton MQTT client is initialized
    token := client.Publish(topic, 0, false, dataJSON)
    token.Wait()

    if token.Error() != nil {
        log.Printf("Failed to publish MQTT message: %v", token.Error())
        http.Error(w, "Failed to publish MQTT message.", http.StatusInternalServerError)
        return
    }

    // Respond to the client
    w.Header().Set("Content-Type", "application/json")
    w.WriteHeader(http.StatusOK)
    response := map[string]string{
        "status": "success",
        "topic":  topic,
        "message": fmt.Sprintf("Message successfully published to topic %s", topic),
    }
    json.NewEncoder(w).Encode(response)
}
// main function initializes the application
func main() {
	// Initialize Supabase client
	initSupabase()

	// Start dynamic scaling of workers
	go monitorAndScaleWorkers()

	// Create the MQTT client
	client := getMQTTClient()

	// Connect to the MQTT broker
	if token := client.Subscribe("/toDaemon/#", 0, nil); token.Wait() && token.Error() != nil {
        log.Fatalf("Failed to subscribe to all topics: %v", token.Error())
    }
    log.Println("Subscribed to all topics!")

	// Handle graceful shutdown
	setupGracefulShutdown(client)

	//init http server

	http.HandleFunc("/sendComand", handleSendCommand)

	fmt.Println("Starting server on :5000...")
	err := http.ListenAndServe(":5000", nil)
	if err != nil {
		fmt.Println("Error starting server:", err)
	}

	// Keep the program running indefinitely
	select {}
}

// messageHandler handles incoming MQTT messages
func messageHandler(client mqtt.Client, msg mqtt.Message) {
	task := Task{
		Topic:   msg.Topic(),
		Payload: string(msg.Payload()),
	}

	select {
	case taskQueue <- task:
		log.Printf("Message enqueued: %s on topic %s", task.Payload, task.Topic)
	default:
		log.Printf("Task queue full. Dropping message: %s on topic %s", task.Payload, task.Topic)
	}
}

// initSupabase initializes the Supabase client as a singleton
func initSupabase() {
	once.Do(func() {
		// Load environment variables from .env file if present
		err := godotenv.Load()
		if err != nil {
			log.Println("Warning: .env file not found, falling back to system environment variables.")
		}

		supabaseURL := os.Getenv("SUPABASE_URL")
		supabaseKey := os.Getenv("SUPABASE_KEY")

		if supabaseURL == "" || supabaseKey == "" {
			log.Fatal("Supabase URL or Key not set in environment variables")
		}

		// Create Supabase client
		supabaseClient = supabase.CreateClient(supabaseURL, supabaseKey)
		log.Println("Connected to Supabase!")
	})
}

// worker processes tasks from the taskQueue
func worker(id int, stopWorkerCh chan bool) {
	log.Printf("Worker %d started", id)
	defer wg.Done()

	batch := []*DeviceLog{}
	batchSize := 10                      // Number of logs per batch
	batchInterval := 5 * time.Second     // Time interval to flush batch
	ticker := time.NewTicker(batchInterval)
	defer ticker.Stop()

	for {
		select {
		case task := <-taskQueue:
			log.Printf("Worker %d processing task: %s on topic %s", id, task.Payload, task.Topic)
			deviceLog, err := parsePayload(task.Topic, task.Payload)
			if err != nil {
				log.Printf("Worker %d failed to parse payload: %v", id, err)
				continue
			}
			batch = append(batch, deviceLog)

			if len(batch) >= batchSize {
				err := batchInsertIntoSupabase(batch)
				if err != nil {
					log.Printf("Worker %d failed to batch insert: %v", id, err)
				} else {
					log.Printf("Worker %d successfully batch inserted %d tasks", id, len(batch))
				}
				batch = []*DeviceLog{} // Reset batch
			}

		case <-ticker.C:
			if len(batch) > 0 {
				err := batchInsertIntoSupabase(batch)
				if err != nil {
					log.Printf("Worker %d failed to batch insert: %v", id, err)
				} else {
					log.Printf("Worker %d successfully batch inserted %d tasks", id, len(batch))
				}
				batch = []*DeviceLog{} // Reset batch
			}

		case <-stopWorkerCh:
			log.Printf("Worker %d stopping", id)
			// Insert any remaining logs before exiting
			if len(batch) > 0 {
				err := batchInsertIntoSupabase(batch)
				if err != nil {
					log.Printf("Worker %d failed to batch insert during shutdown: %v", id, err)
				} else {
					log.Printf("Worker %d successfully batch inserted %d tasks during shutdown", id, len(batch))
				}
			}
			return
		}
	}
}

// parsePayload parses the MQTT payload and returns a DeviceLog struct
func parsePayload(topic, payload string) (*DeviceLog, error) {
	// Attempt to parse payload as JSON
	var rawData map[string]interface{}
	err := json.Unmarshal([]byte(payload), &rawData)
	if err != nil {
		// If payload is not JSON, insert dummy data
		log.Printf("Payload is not JSON: %s. Inserting dummy data.", payload)
		return createDummyDeviceLog(topic, payload), nil
	}

	// Helper function to extract string fields
	getString := func(key string) string {
		if val, exists := rawData[key]; exists {
			if str, ok := val.(string); ok {
				return str
			}
		}
		return "" // Default to empty string if not present or not a string
	}

	// Helper function to extract float fields
	getFloat := func(key string) *float64 {
		if val, exists := rawData[key]; exists {
			switch v := val.(type) {
			case float64:
				return &v
			case float32:
				f := float64(v)
				return &f
			case int:
				f := float64(v)
				return &f
			default:
				return nil
			}
		}
		return nil // Default to nil if not present or not a number
	}

	// Extract fields with defaults
	deviceID := getString("device_id")
	userID := getString("user_id")
	status := getString("status")
	deviceType := getString("device_type")
	relayState := getString("relay_state")

	tempReading := getFloat("temp_sensor_reading")
	humidReading := getFloat("humid_sensor_reading")

	// Optional metadata field
	var metadata *string
	if val, exists := rawData["metadata"]; exists {
		if str, ok := val.(string); ok {
			metadata = &str
		}
	}

	// Validate required fields
	if deviceID == "" || userID == "" || deviceType == "" {
		return nil, errors.New("missing required fields: device_id, user_id, or device_type")
	}

	// Create the DeviceLog struct
	deviceLog := &DeviceLog{
		DeviceID:           deviceID,
		UserID:             userID,
		CreatedAt:          time.Now(),
		Status:             status,
		TempSensorReading:  tempReading,
		HumidSensorReading: humidReading,
		RelayState:         relayState,
		DeviceType:         deviceType,
		Metadata:           metadata,
	}

	return deviceLog, nil
}

// createDummyDeviceLog creates a DeviceLog with dummy data
func createDummyDeviceLog(topic, payload string) *DeviceLog {
	// Extract user_id from topic
	// Assuming the topic format is "/gms/user/{user_id}/log"
	parts := strings.Split(topic, "/")
	userID := ""
	for i, part := range parts {
		if part == "user" && i+1 < len(parts) {
			userID = parts[i+1]
			break
		}
	}

	// If userID is not found, assign a default value
	if userID == "" {
		userID = "default-user-id"
	}

	// Assign a default device_id
	deviceID := defaultDeviceID

	// Assign default values for other fields
	status := payload                     // Using the payload as status
	deviceType := defaultDeviceType       // Defaulting to "relay"; adjust as needed
	relayState := "idle"                  // Default relay state
	var metadata *string                   // No metadata for dummy data

	return &DeviceLog{
		DeviceID:   deviceID,
		UserID:     userID,
		CreatedAt:  time.Now(),
		Status:     status,
		DeviceType: deviceType,
		RelayState: relayState,
		Metadata:   metadata,
		// Other fields remain nil or default
	}
}

// insertIntoSupabase inserts a single DeviceLog into Supabase
func insertIntoSupabase(deviceLog *DeviceLog) error {
	// Convert DeviceLog to a map for insertion
	data := map[string]interface{}{
		"device_id":             deviceLog.DeviceID,
		"user_id":               deviceLog.UserID,
		"created_at":            deviceLog.CreatedAt.Format(time.RFC3339),
		"status":                deviceLog.Status,
		"temp_sensor_reading":   deviceLog.TempSensorReading,
		"humid_sensor_reading":  deviceLog.HumidSensorReading,
		"relay_state":           deviceLog.RelayState,
		"device_type":           deviceLog.DeviceType,
		"metadata":              deviceLog.Metadata,
	}

	// Insert the record
	returnData := supabaseClient.DB.From("deviceLogs").Insert(data).Execute(data)
	

	// Log the response for debugging
	log.Printf("Supabase Insert Response: %+v", returnData)

	return nil
}

// batchInsertIntoSupabase inserts multiple DeviceLogs into Supabase in a single batch
func batchInsertIntoSupabase(deviceLogs []*DeviceLog) error {
	// Convert DeviceLogs to a slice of maps
	var data []map[string]interface{}
	for _, logEntry := range deviceLogs {
		data = append(data, map[string]interface{}{
			"device_id":             logEntry.DeviceID,
			"user_id":               logEntry.UserID,
			"created_at":            logEntry.CreatedAt.Format(time.RFC3339),
			"status":                logEntry.Status,
			"temp_sensor_reading":   logEntry.TempSensorReading,
			"humid_sensor_reading":  logEntry.HumidSensorReading,
			"relay_state":           logEntry.RelayState,
			"device_type":           logEntry.DeviceType,
			"metadata":              logEntry.Metadata,
		})
	}

	// Insert the records
	returnData := supabaseClient.DB.From("deviceLogs").Insert(data).Execute(data)
	

	// Log the response for debugging
	log.Printf("Supabase Batch Insert Response: %+v", returnData)

	return nil
}

const (
	workerScaleThreshold = 0.8 // 80% of maxWorkers
	queueOverloadThreshold = 0.8 // 80% of queue capacity
)

// monitorAndScaleWorkers dynamically scales the number of workers based on the taskQueue size
func monitorAndScaleWorkers() {
	for {
		queueSize := len(taskQueue)
		currentWorkers := len(activeWorkers)
		workerCapacity := float64(maxWorkers) * workerScaleThreshold
		queueCapacity := float64(cap(taskQueue)) * queueOverloadThreshold

		workerLock.Lock()

		// Log if worker count exceeds threshold
		if float64(currentWorkers) > workerCapacity {
			log.Printf("‚ö†Ô∏è WARNING: Worker pool is at %.0f%% capacity (%d/%d workers in use).", (float64(currentWorkers)/float64(maxWorkers))*100, currentWorkers, maxWorkers)
		}

		// Log if worker count hits max capacity
		if currentWorkers >= maxWorkers {
			log.Printf("üö® ALERT: Worker pool has reached max capacity (%d/%d workers). Incoming tasks may experience delays.", currentWorkers, maxWorkers)
		}

		// Scale up workers if needed
		if queueSize > currentWorkers && currentWorkers < maxWorkers {
			workerIDCounter++
			newWorkerID := workerIDCounter
			stopWorkerCh := make(chan bool)
			activeWorkers[newWorkerID] = stopWorkerCh
			wg.Add(1)
			go worker(newWorkerID, stopWorkerCh)
			log.Printf("üîº Scaled up: Started worker %d. Total workers: %d", newWorkerID, len(activeWorkers))
		}

		// Scale down workers if queue is small and above minWorkers
		if queueSize < len(activeWorkers) && len(activeWorkers) > minWorkers {
			for workerID, stopCh := range activeWorkers {
				if len(activeWorkers) > minWorkers {
					close(stopCh)
					delete(activeWorkers, workerID)
					log.Printf("üîΩ Scaled down: Stopped worker %d. Total workers: %d", workerID, len(activeWorkers))
					break
				}
			}
		}

		// Log if queue is nearing capacity
		if float64(queueSize) > queueCapacity {
			log.Printf("‚ö†Ô∏è WARNING: Task queue is at %.0f%% capacity (%d/%d tasks in queue).", (float64(queueSize)/float64(cap(taskQueue)))*100, queueSize, cap(taskQueue))
		}

		// Log if the queue is full
		if queueSize == cap(taskQueue) {
			log.Printf("üö® ALERT: Task queue is FULL (%d tasks). Incoming messages may be dropped.", queueSize)
		}

		workerLock.Unlock()
		time.Sleep(2 * time.Second) // Adjust scaling interval as needed
	}
}

// setupGracefulShutdown handles termination signals to shut down workers gracefully
func setupGracefulShutdown(client mqtt.Client) {
	signal.Notify(gracefulShutdown, syscall.SIGINT, syscall.SIGTERM)

	go func() {
		sig := <-gracefulShutdown
		log.Printf("Received signal: %v. Initiating shutdown...", sig)

		// Disconnect MQTT client
		client.Disconnect(250)
		log.Println("Disconnected from MQTT broker.")

		// Stop all active workers
		workerLock.Lock()
		for workerID, stopCh := range activeWorkers {
			close(stopCh)
			log.Printf("Stopped worker %d", workerID)
		}
		workerLock.Unlock()

		// Wait for all workers to finish
		wg.Wait()
		log.Println("All workers have been stopped.")

		os.Exit(0)
	}()
}
